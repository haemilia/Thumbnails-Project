{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "cnoLVrvotT6l"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import ElectraModel, ElectraTokenizer\n",
        "\n",
        "class LanguageModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LanguageModel, self).__init__()\n",
        "        \n",
        "        # KoElectra model and tokenizer\n",
        "        self.electra = ElectraModel.from_pretrained(\"monologg/koelectra-base-v3-discriminator\")\n",
        "        self.tokenizer = ElectraTokenizer.from_pretrained(\"monologg/koelectra-base-v3-discriminator\")\n",
        "        \n",
        "        # Bi-directional LSTM layer\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=self.electra.config.hidden_size,\n",
        "            hidden_size=50,\n",
        "            num_layers=1,\n",
        "            bidirectional=True,\n",
        "            batch_first=True\n",
        "        )\n",
        "        \n",
        "        # Global Max Pooling and Flatten layers\n",
        "        self.global_max_pooling = nn.AdaptiveMaxPool1d(1)\n",
        "        self.flatten = nn.Flatten()\n",
        "        \n",
        "        # Dense layer\n",
        "        self.dense = nn.Linear(50, 50)\n",
        "        \n",
        "    def forward(self, inputs):\n",
        "        # Tokenize inputs\n",
        "        input_ids = self.tokenizer(\n",
        "            inputs, padding=True, truncation=True, max_length=512, return_tensors=\"pt\"\n",
        "        )[\"input_ids\"]\n",
        "        \n",
        "        # Get KoElectra output\n",
        "        outputs = self.electra(input_ids)\n",
        "        last_hidden_state = outputs.last_hidden_state\n",
        "        \n",
        "        # Pass through LSTM layer\n",
        "        lstm_out, _ = self.lstm(last_hidden_state)\n",
        "        \n",
        "        # Apply Global Max Pooling and Flatten\n",
        "        max_pooled = self.global_max_pooling(lstm_out.permute(0, 2, 1))\n",
        "        flattened = self.flatten(max_pooled)\n",
        "        \n",
        "        # Pass through dense layer\n",
        "        output = self.dense(flattened)\n",
        "        \n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "language_model = LanguageModel()\n",
        "input_string = \"This is a sample input string.\"\n",
        "output_tensor = language_model(input_string)"
      ],
      "metadata": {
        "id": "XteXJXtdtbVy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}