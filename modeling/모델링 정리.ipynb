{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPNllBVp2JV4FG36J0mdLVR"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# NLP model\n","class NLPModel(nn.Module):\n","    def __init__(self, num_classes):\n","        super(NLPModel, self).__init__()\n","        \n","        self.transformer_model = RobertaModel.from_pretrained('roberta-base')\n","        self.lstm = nn.LSTM(input_size=768, hidden_size=50, num_layers=1, bidirectional=True, batch_first=True)\n","        self.fc1 = nn.Linear(100, 50)\n","        self.dropout = nn.Dropout(p=0.2)\n","        self.fc2 = nn.Linear(50, num_classes)\n","        \n","    def forward(self, input_ids, attention_mask):\n","        outputs = self.transformer_model(input_ids, attention_mask=attention_mask)\n","        hidden_states = outputs.last_hidden_state\n","        lstm_out, _ = self.lstm(hidden_states)\n","        max_pool_out, _ = torch.max(lstm_out, 1)\n","        fc1_out = F.relu(self.fc1(max_pool_out))\n","        dropout_out = self.dropout(fc1_out)\n","        output = self.fc2(dropout_out)\n","        return output"],"metadata":{"id":"untSm08tcISD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Vision model\n","class ImgModel(nn.Module):\n","    def __init__(self, num_classes):\n","        super(ImgModel, self).__init__()\n","        \n","        self.base_model = densenet121(pretrained=True)\n","        self.base_model.features.conv0 = nn.Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","        self.base_model.classifier = nn.Identity()\n","        \n","        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n","        self.batch_norm1 = nn.BatchNorm2d(1024)\n","        self.dropout1 = nn.Dropout(p=0.5)\n","        self.fc1 = nn.Linear(1024, 1024)\n","        self.fc2 = nn.Linear(1024, num_classes)\n","        self.batch_norm2 = nn.BatchNorm1d(256)\n","        self.dropout2 = nn.Dropout(p=0.5)\n","        \n","    def forward(self, x):\n","        x = self.base_model.features(x)\n","        x = self.avg_pool(x)\n","        x = x.view(x.size(0), -1)\n","        x = self.batch_norm1(x)\n","        x = F.relu(self.fc1(x))\n","        x = self.dropout1(x)\n","        x = F.relu(self.fc2(x))\n","        x = self.batch_norm2(x)\n","        x = self.dropout2(x)\n","        x = torch.sigmoid(x)\n","        return x"],"metadata":{"id":"SW37Mn3YcH4n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ContextGating model\n","class ContextGating(nn.Module):\n","    def __init__(self, input_dim):\n","        super(ContextGating, self).__init__()\n","        self.input_dim = input_dim\n","        self.sigmoid = nn.Sigmoid()\n","        self.dense = nn.Linear(input_dim, input_dim)\n","        \n","    def forward(self, x, context):\n","        gating_weights = self.sigmoid(self.dense(context))\n","        gated_output = x * gating_weights\n","        return gated_output"],"metadata":{"id":"ZIPKwca0cExW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Context layer 적용 약간 필요 없을 듯?\n","def create_nlp_model():\n","    \n","    # config = RobertaConfig(dropout=0.2, attention_dropout=0.2)\n","    # config.output_hidden_states=False\n","    transformer_model = TFRobertaModel.from_pretrained(pt_model)\n","\n","    in_ids = Input(shape=(100,), name=\"input_token\", dtype=\"int32\")\n","    in_masks = Input(shape=(100,), name=\"masked_token\", dtype=\"int32\")\n","\n","    emb = transformer_model(in_ids, attention_mask=in_masks)[0]\n","    x = Bidirectional(LSTM(50, return_sequences=True, dropout=0.1, recurrent_dropout=0))(emb)\n","    x = GlobalMaxPool1D()(x)\n","    x = Flatten()(x)\n","    x = Dense(50, activation=\"relu\")(x)\n","    x = Dropout(0.2)(x)\n","    \n","    # apply context gating to the output of the model\n","    context_gating = ContextGating(50)\n","    gated_output = context_gating(x, x)\n","    \n","    x = Dense(2, activation=\"softmax\")(gated_output)\n","    model = Model(inputs=[in_ids, in_masks], outputs=x)\n","        \n","    return model"],"metadata":{"id":"wTuwXtJub_uk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def create_img_model():\n","    \n","    inp = Input(shape=(180, 320, 3))\n","    \n","    base_model = DenseNet121(weights=\"imagenet\", include_top=False, input_tensor=inp)\n","    base_model.trainable = False\n","    x = base_model.output\n","    \n","    x = AveragePooling2D()(x)\n","    x = BatchNormalization()(x)\n","    x = Flatten()(x)\n","    x = Dropout(0.5)(x)\n","    x = Dense(1024, activation=\"relu\")(x)\n","    x = Dense(256, activation=\"relu\")(x)\n","    x = BatchNormalization()(x)\n","    x = Dropout(0.5)(x)\n","    \n","    # apply context gating to the output of the model\n","    context_gating = ContextGating(256)\n","    gated_output = context_gating(x, x)\n","    \n","    out = Dense(len(cat_lbl), activation=\"sigmoid\")(gated_output)\n","    \n","    model = Model(base_model.input, out)\n","    \n","    model.compile(loss='binary_crossentropy', \n","                  optimizer=Adam(learning_rate=0.0005),"],"metadata":{"id":"idiHeyHbbxkN"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vD8YgZ7oboBS"},"outputs":[],"source":["#concat 위의 vision 하고 nlp model하고 이 concat 모델 사용하면 될 듯\n","class ConcatModel(nn.Module):\n","    def __init__(self):\n","        super(ConcatModel, self).__init__()\n","        self.nlp_model = create_nlp_model()\n","        self.img_model = create_img_model()\n","        \n","        # Define context gating layer\n","        self.context_gate = nn.Sequential(\n","            nn.Linear(258, 258),\n","            nn.Sigmoid()\n","        )\n","        \n","        # Define dense layer with 20 units\n","        self.dense_layer = nn.Linear(258, 20)\n","        \n","        # Define dropout layer with 0.2 dropout rate\n","        self.dropout_layer = nn.Dropout(0.2)\n","        \n","        # Define final dense layer with softmax activation\n","        self.softmax_layer = nn.Softmax(dim=1)\n","        \n","    def forward(self, nlp_input, img_input):\n","        nlp_output = self.nlp_model(nlp_input)\n","        img_output = self.img_model(img_input)\n","        \n","        # Concatenate the outputs of the two models\n","        concat_output = torch.cat((nlp_output, img_output), dim=1)\n","        \n","        # Apply context gating\n","        gated_output = self.context_gate(concat_output) * concat_output\n","        \n","        # Apply dense layer, dropout layer, and softmax layer\n","        dense_output = self.dense_layer(gated_output)\n","        dropout_output = self.dropout_layer(dense_output)\n","        softmax_output = self.softmax_layer(dropout_output)\n","        \n","        return softmax_output"]}]}